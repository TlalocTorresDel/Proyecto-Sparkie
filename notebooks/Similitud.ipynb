{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "eae6cd9a-9212-4ba0-a7be-06a370459776",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/12/07 19:25:26 WARN Utils: Your hostname, maria-lopez-VirtualBox, resolves to a loopback address: 127.0.1.1; using 10.0.2.15 instead (on interface enp0s3)\n",
      "25/12/07 19:25:26 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/12/07 19:25:28 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - Spark inicializado - \n"
     ]
    }
   ],
   "source": [
    "#Vamos a importar librerías para calcular TF-IDF\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.linalg import Vectors, DenseVector\n",
    "import os\n",
    "import shutil\n",
    "import math\n",
    "\n",
    "# Inicializar Spark\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ProyectoSparkie-Similitudes\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "\n",
    "print(\" - Spark inicializado - \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7a3981b5-aa8f-460c-9771-e1e9e2a7a246",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Primeros 5 registros:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('The_Republic_by_Plato.txt', 'republic', 107)\n",
      "('The_Republic_by_Plato.txt', 'jowett', 2)\n",
      "('The_Republic_by_Plato.txt', 'note', 31)\n",
      "('The_Republic_by_Plato.txt', 'also', 404)\n",
      "('The_Republic_by_Plato.txt', 'ebook', 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:>                                                          (0 + 2) / 2]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "- Total de pares (documento, palabra): 821702\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Aquí se cargan las frecuencias sacadas de Vocabulario luego de aplicar los filtros\n",
    "freq_path = \"../data/processed/frecuencias_rdd\"\n",
    "\n",
    "# Leer el RDD guardado\n",
    "rdd_freq = sc.textFile(freq_path)\n",
    "\n",
    "# Función para parsear el formato para que sean tuplas: \"(('documento.txt', 'palabra'), frecuencia)\"\n",
    "def parse_freq(line):\n",
    "    \"\"\"Convierte string del RDD a tupla Python\"\"\"\n",
    "    # Formato ejemplo: \"(('Doc.txt', 'palabra'), 123)\", 123 son la frecuencia con la que aparece\n",
    "    import ast\n",
    "    parsed = ast.literal_eval(line)\n",
    "    documento = parsed[0][0]\n",
    "    palabra = parsed[0][1]\n",
    "    freq = parsed[1]\n",
    "    return (documento, palabra, freq)\n",
    "\n",
    "rdd_parsed = rdd_freq.map(parse_freq)\n",
    "\n",
    "print(\"- Primeros 5 registros:\")\n",
    "for item in rdd_parsed.take(5):\n",
    "    print(item)\n",
    "    \n",
    "print(f\"\\n- Total de pares (documento, palabra): {rdd_parsed.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "59b10033-1362-43af-a3e2-6aa3ee6ebd18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Total de palabras por documento (primeros 5):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  The_Republic_by_Plato.txt: 93584 palabras\n",
      "  The_2006_CIA_World_Factbook_by_United_States._Central_Intelligence_Agency.txt: 884379 palabras\n",
      "  Little_Women;_Or,_Meg,_Jo,_Beth,_and_Amy_by_Louisa_May_Alcott.txt: 92687 palabras\n",
      "  Middlemarch_by_George_Eliot.txt: 147972 palabras\n",
      "  The_King_in_Yellow_by_Robert_W._Chambers.txt: 34765 palabras\n",
      "\n",
      "- TF calculado (primeros 5):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 5:============================================>              (3 + 1) / 4]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ('Middlemarch_by_George_Eliot.txt', 'george'): TF = 0.000061\n",
      "  ('Middlemarch_by_George_Eliot.txt', 'york'): TF = 0.000027\n",
      "  ('Middlemarch_by_George_Eliot.txt', 'company'): TF = 0.000331\n",
      "  ('Middlemarch_by_George_Eliot.txt', 'dear'): TF = 0.001656\n",
      "  ('Middlemarch_by_George_Eliot.txt', 'henry'): TF = 0.000041\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Aquí se calcula el TF\n",
    "\n",
    "# 1. Calcular total de palabras por documento \n",
    "doc_totals = rdd_parsed.map(lambda x: (x[0], x[2])) \\\n",
    "                       .reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "print(\"- Total de palabras por documento (primeros 5):\")\n",
    "for item in doc_totals.take(5):\n",
    "    print(f\"  {item[0]}: {item[1]} palabras\")\n",
    "\n",
    "# 2. Calcular TF: freq / total_palabras_doc, la formula que nos dio el profesor de TF = Numero de apariciones del termino N en documento M / Numero de tockens en el doc j\n",
    "# Para recordar, la estructura es así: ((doc, palabra), freq)\n",
    "rdd_doc_word_freq = rdd_parsed.map(lambda x: ((x[0], x[1]), x[2]))\n",
    "\n",
    "# Join con totales: ((doc, palabra), (freq, total))\n",
    "rdd_with_totals = rdd_doc_word_freq.map(lambda x: (x[0][0], (x[0][1], x[1]))) \\\n",
    "                                   .join(doc_totals) \\\n",
    "                                   .map(lambda x: ((x[0], x[1][0][0]), (x[1][0][1], x[1][1])))\n",
    "\n",
    "# Calcular con la formula del profe TF = freq / total\n",
    "rdd_tf = rdd_with_totals.map(lambda x: (x[0], x[1][0] / x[1][1]))\n",
    "\n",
    "print(\"\\n- TF calculado (primeros 5):\")\n",
    "for item in rdd_tf.take(5):\n",
    "    print(f\"  {item[0]}: TF = {item[1]:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "143388c8-c424-4dbc-a310-4f94120dfaec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Total de documentos: 99\n",
      "\n",
      "- Documentos por palabra (primeros 10):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 10:>                                                         (0 + 2) / 2]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  'plato': aparece en 23 documentos\n",
      "  'translated': aparece en 50 documentos\n",
      "  'benjamin': aparece en 16 documentos\n",
      "  'see': aparece en 97 documentos\n",
      "  '150': aparece en 24 documentos\n",
      "  'contents': aparece en 87 documentos\n",
      "  'introduction': aparece en 67 documentos\n",
      "  'analysis': aparece en 33 documentos\n",
      "  'persons': aparece en 81 documentos\n",
      "  'iii': aparece en 77 documentos\n",
      "\n",
      "- IDF calculado (primeros 10):\n",
      "  'plato': IDF = 1.4596\n",
      "  'translated': IDF = 0.6831\n",
      "  'benjamin': IDF = 1.8225\n",
      "  'see': IDF = 0.0204\n",
      "  '150': IDF = 1.4171\n",
      "  'contents': IDF = 0.1292\n",
      "  'introduction': IDF = 0.3904\n",
      "  'analysis': IDF = 1.0986\n",
      "  'persons': IDF = 0.2007\n",
      "  'iii': IDF = 0.2513\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Calcular ahora el IDF:\n",
    "# IDF o peso = log(total_documentos / documentos_que_contienen_palabra)\n",
    "\n",
    "# 1. Contar total de documentos para tenerlo en una variable\n",
    "total_docs = doc_totals.count()\n",
    "print(f\"- Total de documentos: {total_docs}\")\n",
    "\n",
    "# 2. Contar en cuántos documentos aparece cada palabra\n",
    "# Estructura así queda: (palabra, 1) -> (palabra, num_docs)\n",
    "rdd_word_doc_count = rdd_parsed.map(lambda x: (x[1], x[0])) \\\n",
    "                               .distinct() \\\n",
    "                               .map(lambda x: (x[0], 1)) \\\n",
    "                               .reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "#Comprobar que se hizo bien:\n",
    "print(\"\\n- Documentos por palabra (primeros 10):\")\n",
    "for item in rdd_word_doc_count.take(10):\n",
    "    print(f\"  '{item[0]}': aparece en {item[1]} documentos\")\n",
    "\n",
    "# Ahora sí calcular IDF\n",
    "# 3. Calcular IDF\n",
    "rdd_idf = rdd_word_doc_count.map(lambda x: (x[0], math.log(total_docs / x[1])))\n",
    "\n",
    "#Verificamos que todo bien:\n",
    "print(\"\\n- IDF calculado (primeros 10):\")\n",
    "for item in rdd_idf.take(10):\n",
    "    print(f\"  '{item[0]}': IDF = {item[1]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ea90cd41-c40d-427f-9671-8afafb7ee7e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- TF-IDF calculado (primeros 10):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Doc: Middlemarch_by_George_Eliot.txt... | Palabra: 'boston' | TF-IDF: 0.000010\n",
      "  Doc: Golden_Days_for_Boys_and_Girls,_Vol._XII... | Palabra: 'boston' | TF-IDF: 0.000426\n",
      "  Doc: Precious_balms.txt... | Palabra: 'boston' | TF-IDF: 0.000322\n",
      "  Doc: War_and_Peace_by_graf_Leo_Tolstoy.txt... | Palabra: 'boston' | TF-IDF: 0.000051\n",
      "  Doc: How_to_Observe__Morals_and_Manners_by_Ha... | Palabra: 'boston' | TF-IDF: 0.000049\n",
      "  Doc: The_Great_Gatsby_by_F._Scott_Fitzgerald.... | Palabra: 'boston' | TF-IDF: 0.000067\n",
      "  Doc: The_2003_CIA_World_Factbook_by_United_St... | Palabra: 'boston' | TF-IDF: 0.000100\n",
      "  Doc: Narrative_of_the_Life_of_Frederick_Dougl... | Palabra: 'boston' | TF-IDF: 0.000246\n",
      "  Doc: The_Scarlet_Letter_by_Nathaniel_Hawthorn... | Palabra: 'boston' | TF-IDF: 0.000407\n",
      "  Doc: The_Souls_of_Black_Folk_by_W._E._B._Du_B... | Palabra: 'boston' | TF-IDF: 0.000128\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 26:======================================>                   (4 + 2) / 6]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "- TF-IDF guardado en: ../data/processed/tfidf_rdd con rotundo exito\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Ahora sí calculamos el TF-IDF = TF * IDF\n",
    "\n",
    "# Preparar TF, lo transformamos: ((doc, palabra), tf) -> (palabra, (doc, tf))\n",
    "rdd_tf_prep = rdd_tf.map(lambda x: (x[0][1], (x[0][0], x[1])))\n",
    "\n",
    "# Join con IDF: (palabra, ((doc, tf), idf))\n",
    "rdd_tf_idf = rdd_tf_prep.join(rdd_idf) \\\n",
    "                        .map(lambda x: ((x[1][0][0], x[0]), x[1][0][1] * x[1][1]))\n",
    "# Verificamos\n",
    "print(\"- TF-IDF calculado (primeros 10):\")\n",
    "for item in rdd_tf_idf.take(10):\n",
    "    print(f\"  Doc: {item[0][0][:40]}... | Palabra: '{item[0][1]}' | TF-IDF: {item[1]:.6f}\")\n",
    "\n",
    "# Guardar TF-IDF en nuestra carpetita de procesados\n",
    "output_tfidf = \"../data/processed/tfidf_rdd\"\n",
    "if os.path.exists(output_tfidf):\n",
    "    shutil.rmtree(output_tfidf)\n",
    "rdd_tf_idf.saveAsTextFile(output_tfidf)\n",
    "\n",
    "print(f\"\\n- TF-IDF guardado en: {output_tfidf} con rotundo exito\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6359f63b-120d-4740-ab55-6d6c35b8076c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Tamaño del vocabulario: 120381 palabras únicas\n",
      "\n",
      "- Primeras 10 palabras del vocabulario:\n",
      "  'boston': índice 0\n",
      "  'husband': índice 1\n",
      "  'blessed': índice 2\n",
      "  'miss': índice 3\n",
      "  'iii': índice 4\n",
      "  'old': índice 5\n",
      "  'xxi': índice 6\n",
      "  'xxv': índice 7\n",
      "  'three': índice 8\n",
      "  'love': índice 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 46:================================================>         (5 + 1) / 6]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "- Vectores creados para 99 documentos\n",
      "\n",
      "- Primer documento (primeras 10 dimensiones):\n",
      "  Documento: Golden_Days_for_Boys_and_Girls,_Vol._XII,_Jan._3,_...\n",
      "  Vector (primeros 10): [(0, 0.00042555406127492234), (1, 1.5735175681184916e-05), (2, 2.717984851291704e-05), (3, 2.388931302330423e-05), (4, 1.9706298775261205e-05), (5, 0.00011965729363756082), (8, 8.127510054824112e-05), (9, 3.30354989483807e-05), (19, 4.7205527043554754e-05), (20, 2.8308111466896454e-05)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Convertir TF-IDF a vectores por documento para usarlos en pasos posteriores\n",
    "\n",
    "# 1. Crear vocabulario global con índices\n",
    "vocab_global = rdd_tf_idf.map(lambda x: x[0][1]).distinct().zipWithIndex()\n",
    "vocab_dict = vocab_global.collectAsMap()\n",
    "vocab_size = len(vocab_dict)\n",
    "\n",
    "#Imprimimos para ver que todo esté bien:\n",
    "print(f\"- Tamaño del vocabulario: {vocab_size} palabras únicas\")\n",
    "print(f\"\\n- Primeras 10 palabras del vocabulario:\")\n",
    "for word, idx in list(vocab_dict.items())[:10]:\n",
    "    print(f\"  '{word}': índice {idx}\")\n",
    "\n",
    "# 2. Convertir TF-IDF a formato (doc, [(idx, tfidf), ...])\n",
    "#Formato: (documento, [(índice_palabra, tfidf), ...])\n",
    "rdd_doc_vectors = rdd_tf_idf.map(lambda x: (x[0][0], (vocab_dict[x[0][1]], x[1]))) \\\n",
    "                            .groupByKey() \\\n",
    "                            .map(lambda x: (x[0], list(x[1])))\n",
    "\n",
    "print(f\"\\n- Vectores creados para {rdd_doc_vectors.count()} documentos\")\n",
    "print(\"\\n- Primer documento (primeras 10 dimensiones):\")\n",
    "first_doc = rdd_doc_vectors.first()\n",
    "print(f\"  Documento: {first_doc[0][:50]}...\")\n",
    "print(f\"  Vector (primeros 10): {first_doc[1][:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "250df88d-1ec2-449f-946d-7ab6e5813348",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Vectores recolectados: 99 documentos\n"
     ]
    }
   ],
   "source": [
    "#Calculamos ahora la similitud por coseno:\n",
    "\n",
    "def cosine_similarity_sparse(vec1, vec2):\n",
    "    \"\"\"\n",
    "    Calcula similitud coseno entre dos vectores dispersos\n",
    "    vec1, vec2: lista de tuplas [(índice, valor), ...]\n",
    "    \"\"\"\n",
    "    # Convertir a diccionarios para búsqueda rápida\n",
    "    dict1 = dict(vec1)\n",
    "    dict2 = dict(vec2)\n",
    "    \n",
    "    # Producto punto\n",
    "    dot_product = sum(dict1.get(idx, 0) * dict2.get(idx, 0) \n",
    "                      for idx in set(dict1.keys()) | set(dict2.keys()))\n",
    "    \n",
    "    # Normas (La parte de abajo de la formula [||V||*||W||])\n",
    "    norm1 = math.sqrt(sum(v**2 for v in dict1.values()))\n",
    "    norm2 = math.sqrt(sum(v**2 for v in dict2.values()))\n",
    "    \n",
    "    if norm1 == 0 or norm2 == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    return dot_product / (norm1 * norm2)\n",
    "\n",
    "# Broadcast de vectores para comparación eficiente\n",
    "doc_vectors_list = rdd_doc_vectors.collect()\n",
    "print(f\"- Vectores recolectados: {len(doc_vectors_list)} documentos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a356a841-ae68-485e-ba01-0ec2124293b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Calculando similitudes entre todos los pares...\n",
      "- Total de pares a calcular: 4851\n",
      "  Procesados: 10/99 documentos...\n",
      "  Procesados: 20/99 documentos...\n",
      "  Procesados: 30/99 documentos...\n",
      "  Procesados: 40/99 documentos...\n",
      "  Procesados: 50/99 documentos...\n",
      "  Procesados: 60/99 documentos...\n",
      "  Procesados: 70/99 documentos...\n",
      "  Procesados: 80/99 documentos...\n",
      "  Procesados: 90/99 documentos...\n",
      "\n",
      "- Similitudes calculadas: 4851 pares\n",
      "\n",
      "- Top 10 pares más similares:\n",
      "  0.9990 | A_Christmas_Carol_in_Prose;_Being_a_Ghos... ↔ A_Christmas_Carol_by_Charles_Dickens.txt...\n",
      "  0.9958 | Little_Women;_Or,_Meg,_Jo,_Beth,_and_Amy... ↔ Little_Women_by_Louisa_May_Alcott.txt...\n",
      "  0.5293 | The_Confessions_of_St._Augustine_by_Bish... ↔ Paradise_Lost_by_John_Milton.txt...\n",
      "  0.5235 | The_2003_CIA_World_Factbook_by_United_St... ↔ The_2006_CIA_World_Factbook_by_United_St...\n",
      "  0.4415 | The_Adventures_of_Sherlock_Holmes_by_Art... ↔ The_Hound_of_the_Baskervilles_by_Arthur_...\n",
      "  0.4249 | The_Confessions_of_St._Augustine_by_Bish... ↔ The_divine_comedy_by_Dante_Alighieri.txt...\n",
      "  0.4188 | The_divine_comedy_by_Dante_Alighieri.txt... ↔ Paradise_Lost_by_John_Milton.txt...\n",
      "  0.4090 | A_Study_in_Scarlet_by_Arthur_Conan_Doyle... ↔ The_Adventures_of_Sherlock_Holmes_by_Art...\n",
      "  0.3596 | The_Iliad_by_Homer.txt... ↔ The_Aeneid_by_Virgil.txt...\n",
      "  0.3507 | Adventures_of_Huckleberry_Finn_by_Mark_T... ↔ The_Adventures_of_Tom_Sawyer,_Complete_b...\n"
     ]
    }
   ],
   "source": [
    "# Crear todas las combinaciones de pares de documentos\n",
    "from itertools import combinations\n",
    "\n",
    "print(\"- Calculando similitudes entre todos los pares...\")\n",
    "\n",
    "# Generar pares y calcular similitudes\n",
    "similarities = []\n",
    "doc_names = [doc[0] for doc in doc_vectors_list]\n",
    "total_pairs = len(doc_names) * (len(doc_names) - 1) // 2\n",
    "\n",
    "print(f\"- Total de pares a calcular: {total_pairs}\")\n",
    "\n",
    "#Comparamos cada documento con todos los demás\n",
    "for i, (doc1, vec1) in enumerate(doc_vectors_list):\n",
    "    for doc2, vec2 in doc_vectors_list[i+1:]:\n",
    "        sim = cosine_similarity_sparse(vec1, vec2)\n",
    "        similarities.append((doc1, doc2, sim))\n",
    "    \n",
    "    if (i + 1) % 10 == 0:\n",
    "        print(f\"  Procesados: {i+1}/{len(doc_vectors_list)} documentos...\")\n",
    "\n",
    "# Convertir a RDD para guardarlo después\n",
    "rdd_similarities = sc.parallelize(similarities)\n",
    "\n",
    "print(f\"\\n- Similitudes calculadas: {rdd_similarities.count()} pares\")\n",
    "print(\"\\n- Top 10 pares más similares:\")\n",
    "\n",
    "#Guardamos: (doc1, doc2, similitud)\n",
    "top_similar = rdd_similarities.sortBy(lambda x: x[2], ascending=False).take(10)\n",
    "for doc1, doc2, sim in top_similar:\n",
    "    print(f\"  {sim:.4f} | {doc1[:40]}... ↔ {doc2[:40]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "522db020-93c1-4018-a4a6-9f2e40373e62",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Matriz de similitudes guardada en: ../data/processed/similarity_matrix_rdd\n"
     ]
    }
   ],
   "source": [
    "# Guardar matriz ahora sí en nuestra carpeta de procesados\n",
    "output_sim = \"../data/processed/similarity_matrix_rdd\"\n",
    "if os.path.exists(output_sim):\n",
    "    shutil.rmtree(output_sim)\n",
    "\n",
    "rdd_similarities.saveAsTextFile(output_sim)\n",
    "print(f\"- Matriz de similitudes guardada en: {output_sim}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "a8ce3ad0-cbe3-4c0a-8098-1230ae029831",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844a20d6-6cee-4125-b24d-5bc5fc1bb264",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
