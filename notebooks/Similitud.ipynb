{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "eae6cd9a-9212-4ba0-a7be-06a370459776",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " - Spark inicializado - \n"
     ]
    }
   ],
   "source": [
    "#Vamos a importar librerías para calcular TF-IDF\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.linalg import Vectors, DenseVector\n",
    "import os\n",
    "import shutil\n",
    "import math\n",
    "\n",
    "# Inicializar Spark\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ProyectoSparkie-Similitudes\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext\n",
    "\n",
    "print(\" - Spark inicializado - \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7a3981b5-aa8f-460c-9771-e1e9e2a7a246",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Primeros 5 registros:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Romeo_and_Juliet_by_William_Shakespeare.txt', 'tragedy', 1)\n",
      "('Romeo_and_Juliet_by_William_Shakespeare.txt', 'romeo', 316)\n",
      "('Romeo_and_Juliet_by_William_Shakespeare.txt', 'william', 1)\n",
      "('Romeo_and_Juliet_by_William_Shakespeare.txt', 'shakespeare', 1)\n",
      "('Romeo_and_Juliet_by_William_Shakespeare.txt', 'contents', 1)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 1:=============================>                             (1 + 1) / 2]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "- Total de pares (documento, palabra): 821702\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Aquí se cargan las frecuencias sacadas de Vocabulario luego de aplicar los filtros\n",
    "freq_path = \"../data/processed/frecuencias_rdd\"\n",
    "\n",
    "# Leer el RDD guardado\n",
    "rdd_freq = sc.textFile(freq_path)\n",
    "\n",
    "# Función para parsear el formato para que sean tuplas: \"(('documento.txt', 'palabra'), frecuencia)\"\n",
    "def parse_freq(line):\n",
    "    \"\"\"Convierte string del RDD a tupla Python\"\"\"\n",
    "    # Formato ejemplo: \"(('Doc.txt', 'palabra'), 123)\", 123 son la frecuencia con la que aparece\n",
    "    import ast\n",
    "    parsed = ast.literal_eval(line)\n",
    "    documento = parsed[0][0]\n",
    "    palabra = parsed[0][1]\n",
    "    freq = parsed[1]\n",
    "    return (documento, palabra, freq)\n",
    "\n",
    "rdd_parsed = rdd_freq.map(parse_freq)\n",
    "\n",
    "print(\"- Primeros 5 registros:\")\n",
    "for item in rdd_parsed.take(5):\n",
    "    print(item)\n",
    "    \n",
    "print(f\"\\n- Total de pares (documento, palabra): {rdd_parsed.count()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "59b10033-1362-43af-a3e2-6aa3ee6ebd18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Total de palabras por documento (primeros 5):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Moby_Dick;_Or,_The_Whale_by_Herman_Melville.txt: 109524 palabras\n",
      "  Peter_Pan___by_J._M._Barrie.txt: 21561 palabras\n",
      "  Thus_Spake_Zarathustra__A_Book_for_All_and_None_by_Friedrich_Wilhelm_Nietzsche.txt: 56548 palabras\n",
      "  A_Christmas_Carol_by_Charles_Dickens.txt: 14192 palabras\n",
      "  The_Tragical_History_of_Doctor_Faustus_by_Christopher_Marlowe.txt: 10813 palabras\n",
      "\n",
      "- TF calculado (primeros 5):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 6:>                                                          (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  ('Peter_Pan___by_J._M._Barrie.txt', 'barrie'): TF = 0.000093\n",
      "  ('Peter_Pan___by_J._M._Barrie.txt', 'matthew'): TF = 0.000046\n",
      "  ('Peter_Pan___by_J._M._Barrie.txt', 'produced'): TF = 0.000046\n",
      "  ('Peter_Pan___by_J._M._Barrie.txt', '1991'): TF = 0.000046\n",
      "  ('Peter_Pan___by_J._M._Barrie.txt', 'duncan'): TF = 0.000046\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Aquí se calcula el TF\n",
    "\n",
    "# 1. Calcular total de palabras por documento \n",
    "doc_totals = rdd_parsed.map(lambda x: (x[0], x[2])) \\\n",
    "                       .reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "print(\"- Total de palabras por documento (primeros 5):\")\n",
    "for item in doc_totals.take(5):\n",
    "    print(f\"  {item[0]}: {item[1]} palabras\")\n",
    "\n",
    "# 2. Calcular TF: freq / total_palabras_doc, la formula que nos dio el profesor de TF = Numero de apariciones del termino N en documento M / Numero de tockens en el doc j\n",
    "# Para recordar, la estructura es así: ((doc, palabra), freq)\n",
    "rdd_doc_word_freq = rdd_parsed.map(lambda x: ((x[0], x[1]), x[2]))\n",
    "\n",
    "# Join con totales: ((doc, palabra), (freq, total))\n",
    "rdd_with_totals = rdd_doc_word_freq.map(lambda x: (x[0][0], (x[0][1], x[1]))) \\\n",
    "                                   .join(doc_totals) \\\n",
    "                                   .map(lambda x: ((x[0], x[1][0][0]), (x[1][0][1], x[1][1])))\n",
    "\n",
    "# Calcular con la formula del profe TF = freq / total\n",
    "rdd_tf = rdd_with_totals.map(lambda x: (x[0], x[1][0] / x[1][1]))\n",
    "\n",
    "print(\"\\n- TF calculado (primeros 5):\")\n",
    "for item in rdd_tf.take(5):\n",
    "    print(f\"  {item[0]}: TF = {item[1]:.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "143388c8-c424-4dbc-a310-4f94120dfaec",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Total de documentos: 99\n",
      "\n",
      "- Documentos por palabra (primeros 10):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  'moby': aparece en 2 documentos\n",
      "  'herman': aparece en 5 documentos\n",
      "  'melville': aparece en 2 documentos\n",
      "  'contents': aparece en 87 documentos\n",
      "  'etymology': aparece en 6 documentos\n",
      "  'supplied': aparece en 62 documentos\n",
      "  'sub': aparece en 24 documentos\n",
      "  'chapter': aparece en 71 documentos\n",
      "  'carpet': aparece en 49 documentos\n",
      "  'bag': aparece en 59 documentos\n",
      "\n",
      "- IDF calculado (primeros 10):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 14:>                                                         (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  'moby': IDF = 3.9020\n",
      "  'herman': IDF = 2.9857\n",
      "  'melville': IDF = 3.9020\n",
      "  'contents': IDF = 0.1292\n",
      "  'etymology': IDF = 2.8034\n",
      "  'supplied': IDF = 0.4680\n",
      "  'sub': IDF = 1.4171\n",
      "  'chapter': IDF = 0.3324\n",
      "  'carpet': IDF = 0.7033\n",
      "  'bag': IDF = 0.5176\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Calcular ahora el IDF:\n",
    "# IDF o peso = log(total_documentos / documentos_que_contienen_palabra)\n",
    "\n",
    "# 1. Contar total de documentos para tenerlo en una variable\n",
    "total_docs = doc_totals.count()\n",
    "print(f\"- Total de documentos: {total_docs}\")\n",
    "\n",
    "# 2. Contar en cuántos documentos aparece cada palabra\n",
    "# Estructura así queda: (palabra, 1) -> (palabra, num_docs)\n",
    "rdd_word_doc_count = rdd_parsed.map(lambda x: (x[1], x[0])) \\\n",
    "                               .distinct() \\\n",
    "                               .map(lambda x: (x[0], 1)) \\\n",
    "                               .reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "#Comprobar que se hizo bien:\n",
    "print(\"\\n- Documentos por palabra (primeros 10):\")\n",
    "for item in rdd_word_doc_count.take(10):\n",
    "    print(f\"  '{item[0]}': aparece en {item[1]} documentos\")\n",
    "\n",
    "# Ahora sí calcular IDF\n",
    "# 3. Calcular IDF\n",
    "rdd_idf = rdd_word_doc_count.map(lambda x: (x[0], math.log(total_docs / x[1])))\n",
    "\n",
    "#Verificamos que todo bien:\n",
    "print(\"\\n- IDF calculado (primeros 10):\")\n",
    "for item in rdd_idf.take(10):\n",
    "    print(f\"  '{item[0]}': IDF = {item[1]:.4f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ea90cd41-c40d-427f-9671-8afafb7ee7e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- TF-IDF calculado (primeros 10):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Doc: Peter_Pan___by_J._M._Barrie.txt... | Palabra: 'pan' | TF-IDF: 0.000978\n",
      "  Doc: My_Life_—_Volume_1_by_Richard_Wagner.txt... | Palabra: 'pan' | TF-IDF: 0.000023\n",
      "  Doc: The_Interesting_Narrative_of_the_Life_of... | Palabra: 'pan' | TF-IDF: 0.000066\n",
      "  Doc: Alice's_Adventures_in_Wonderland_by_Lewi... | Palabra: 'pan' | TF-IDF: 0.000067\n",
      "  Doc: Precious_balms.txt... | Palabra: 'pan' | TF-IDF: 0.001809\n",
      "  Doc: Anna_Karenina_by_graf_Leo_Tolstoy.txt... | Palabra: 'pan' | TF-IDF: 0.000005\n",
      "  Doc: The_Adventures_of_Roderick_Random_by_T._... | Palabra: 'pan' | TF-IDF: 0.000009\n",
      "  Doc: Leviathan_by_Thomas_Hobbes.txt... | Palabra: 'pan' | TF-IDF: 0.000026\n",
      "  Doc: The_Adventures_of_Tom_Sawyer,_Complete_b... | Palabra: 'pan' | TF-IDF: 0.000024\n",
      "  Doc: The_Aeneid_by_Virgil.txt... | Palabra: 'pan' | TF-IDF: 0.000013\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "- TF-IDF guardado en: ../data/processed/tfidf_rdd con rotundo exito\n"
     ]
    }
   ],
   "source": [
    "# Ahora sí calculamos el TF-IDF = TF * IDF\n",
    "\n",
    "# Preparar TF, lo transformamos: ((doc, palabra), tf) -> (palabra, (doc, tf))\n",
    "rdd_tf_prep = rdd_tf.map(lambda x: (x[0][1], (x[0][0], x[1])))\n",
    "\n",
    "# Join con IDF: (palabra, ((doc, tf), idf))\n",
    "rdd_tf_idf = rdd_tf_prep.join(rdd_idf) \\\n",
    "                        .map(lambda x: ((x[1][0][0], x[0]), x[1][0][1] * x[1][1]))\n",
    "# Verificamos\n",
    "print(\"- TF-IDF calculado (primeros 10):\")\n",
    "for item in rdd_tf_idf.take(10):\n",
    "    print(f\"  Doc: {item[0][0][:40]}... | Palabra: '{item[0][1]}' | TF-IDF: {item[1]:.6f}\")\n",
    "\n",
    "# Guardar TF-IDF en nuestra carpetita de procesados\n",
    "output_tfidf = \"../data/processed/tfidf_rdd\"\n",
    "if os.path.exists(output_tfidf):\n",
    "    shutil.rmtree(output_tfidf)\n",
    "rdd_tf_idf.saveAsTextFile(output_tfidf)\n",
    "\n",
    "print(f\"\\n- TF-IDF guardado en: {output_tfidf} con rotundo exito\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6359f63b-120d-4740-ab55-6d6c35b8076c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Tamaño del vocabulario: 120381 palabras únicas\n",
      "\n",
      "- Primeras 10 palabras del vocabulario:\n",
      "  'pan': índice 0\n",
      "  'wendy': índice 1\n",
      "  'james': índice 2\n",
      "  'millennium': índice 3\n",
      "  'text': índice 4\n",
      "  'iii': índice 5\n",
      "  'island': índice 6\n",
      "  'mermaids': índice 7\n",
      "  'lagoon': índice 8\n",
      "  'children': índice 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "- Vectores creados para 99 documentos\n",
      "\n",
      "- Primer documento (primeras 10 dimensiones):\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 54:>                                                         (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  Documento: Alice's_Adventures_in_Wonderland_by_Lewis_Carroll....\n",
      "  Vector (primeros 10): [(0, 6.669931043068998e-05), (3, 0.0001807225347373104), (5, 4.134140948855176e-05), (9, 6.930444449559121e-05), (11, 6.445272348634077e-05), (12, 3.301047795067466e-05), (13, 7.839297565744772e-06), (14, 0.00021513332675391212), (16, 6.685019252815768e-05), (17, 5.656554752382573e-05)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Convertir TF-IDF a vectores por documento para usarlos en pasos posteriores\n",
    "\n",
    "# 1. Crear vocabulario global con índices\n",
    "vocab_global = rdd_tf_idf.map(lambda x: x[0][1]).distinct().zipWithIndex()\n",
    "vocab_dict = vocab_global.collectAsMap()\n",
    "vocab_size = len(vocab_dict)\n",
    "\n",
    "#Imprimimos para ver que todo esté bien:\n",
    "print(f\"- Tamaño del vocabulario: {vocab_size} palabras únicas\")\n",
    "print(f\"\\n- Primeras 10 palabras del vocabulario:\")\n",
    "for word, idx in list(vocab_dict.items())[:10]:\n",
    "    print(f\"  '{word}': índice {idx}\")\n",
    "\n",
    "# 2. Convertir TF-IDF a formato (doc, [(idx, tfidf), ...])\n",
    "#Formato: (documento, [(índice_palabra, tfidf), ...])\n",
    "rdd_doc_vectors = rdd_tf_idf.map(lambda x: (x[0][0], (vocab_dict[x[0][1]], x[1]))) \\\n",
    "                            .groupByKey() \\\n",
    "                            .map(lambda x: (x[0], list(x[1])))\n",
    "\n",
    "print(f\"\\n- Vectores creados para {rdd_doc_vectors.count()} documentos\")\n",
    "print(\"\\n- Primer documento (primeras 10 dimensiones):\")\n",
    "first_doc = rdd_doc_vectors.first()\n",
    "print(f\"  Documento: {first_doc[0][:50]}...\")\n",
    "print(f\"  Vector (primeros 10): {first_doc[1][:10]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "250df88d-1ec2-449f-946d-7ab6e5813348",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Vectores recolectados: 99 documentos\n"
     ]
    }
   ],
   "source": [
    "#Calculamos ahora la similitud por coseno:\n",
    "\n",
    "def cosine_similarity_sparse(vec1, vec2):\n",
    "    \"\"\"\n",
    "    Calcula similitud coseno entre dos vectores dispersos\n",
    "    vec1, vec2: lista de tuplas [(índice, valor), ...]\n",
    "    \"\"\"\n",
    "    # Convertir a diccionarios para búsqueda rápida\n",
    "    dict1 = dict(vec1)\n",
    "    dict2 = dict(vec2)\n",
    "    \n",
    "    # Producto punto\n",
    "    dot_product = sum(dict1.get(idx, 0) * dict2.get(idx, 0) \n",
    "                      for idx in set(dict1.keys()) | set(dict2.keys()))\n",
    "    \n",
    "    # Normas (La parte de abajo de la formula [||V||*||W||])\n",
    "    norm1 = math.sqrt(sum(v**2 for v in dict1.values()))\n",
    "    norm2 = math.sqrt(sum(v**2 for v in dict2.values()))\n",
    "    \n",
    "    if norm1 == 0 or norm2 == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    return dot_product / (norm1 * norm2)\n",
    "\n",
    "# Broadcast de vectores para comparación eficiente\n",
    "doc_vectors_list = rdd_doc_vectors.collect()\n",
    "print(f\"- Vectores recolectados: {len(doc_vectors_list)} documentos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "a356a841-ae68-485e-ba01-0ec2124293b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Calculando similitudes entre todos los pares...\n",
      "- Total de pares a calcular: 4851\n",
      "  Procesados: 10/99 documentos...\n",
      "  Procesados: 20/99 documentos...\n",
      "  Procesados: 30/99 documentos...\n",
      "  Procesados: 40/99 documentos...\n",
      "  Procesados: 50/99 documentos...\n",
      "  Procesados: 60/99 documentos...\n",
      "  Procesados: 70/99 documentos...\n",
      "  Procesados: 80/99 documentos...\n",
      "  Procesados: 90/99 documentos...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "- Similitudes calculadas: 4851 pares\n",
      "\n",
      "- Top 10 pares más similares:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 66:>                                                         (0 + 1) / 1]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0.9990 | A_Christmas_Carol_in_Prose;_Being_a_Ghos... ↔ A_Christmas_Carol_by_Charles_Dickens.txt...\n",
      "  0.9958 | Little_Women;_Or,_Meg,_Jo,_Beth,_and_Amy... ↔ Little_Women_by_Louisa_May_Alcott.txt...\n",
      "  0.5293 | The_Confessions_of_St._Augustine_by_Bish... ↔ Paradise_Lost_by_John_Milton.txt...\n",
      "  0.5235 | The_2003_CIA_World_Factbook_by_United_St... ↔ The_2006_CIA_World_Factbook_by_United_St...\n",
      "  0.4415 | The_Adventures_of_Sherlock_Holmes_by_Art... ↔ The_Hound_of_the_Baskervilles_by_Arthur_...\n",
      "  0.4249 | The_Confessions_of_St._Augustine_by_Bish... ↔ The_divine_comedy_by_Dante_Alighieri.txt...\n",
      "  0.4188 | The_divine_comedy_by_Dante_Alighieri.txt... ↔ Paradise_Lost_by_John_Milton.txt...\n",
      "  0.4090 | A_Study_in_Scarlet_by_Arthur_Conan_Doyle... ↔ The_Adventures_of_Sherlock_Holmes_by_Art...\n",
      "  0.3596 | The_Iliad_by_Homer.txt... ↔ The_Aeneid_by_Virgil.txt...\n",
      "  0.3507 | Adventures_of_Huckleberry_Finn_by_Mark_T... ↔ The_Adventures_of_Tom_Sawyer,_Complete_b...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    }
   ],
   "source": [
    "# Crear todas las combinaciones de pares de documentos\n",
    "from itertools import combinations\n",
    "\n",
    "print(\"- Calculando similitudes entre todos los pares...\")\n",
    "\n",
    "# Generar pares y calcular similitudes\n",
    "similarities = []\n",
    "doc_names = [doc[0] for doc in doc_vectors_list]\n",
    "total_pairs = len(doc_names) * (len(doc_names) - 1) // 2\n",
    "\n",
    "print(f\"- Total de pares a calcular: {total_pairs}\")\n",
    "\n",
    "#Comparamos cada documento con todos los demás\n",
    "for i, (doc1, vec1) in enumerate(doc_vectors_list):\n",
    "    for doc2, vec2 in doc_vectors_list[i+1:]:\n",
    "        sim = cosine_similarity_sparse(vec1, vec2)\n",
    "        similarities.append((doc1, doc2, sim))\n",
    "    \n",
    "    if (i + 1) % 10 == 0:\n",
    "        print(f\"  Procesados: {i+1}/{len(doc_vectors_list)} documentos...\")\n",
    "\n",
    "# Convertir a RDD para guardarlo después\n",
    "rdd_similarities = sc.parallelize(similarities)\n",
    "\n",
    "print(f\"\\n- Similitudes calculadas: {rdd_similarities.count()} pares\")\n",
    "print(\"\\n- Top 10 pares más similares:\")\n",
    "\n",
    "#Guardamos: (doc1, doc2, similitud)\n",
    "top_similar = rdd_similarities.sortBy(lambda x: x[2], ascending=False).take(10)\n",
    "for doc1, doc2, sim in top_similar:\n",
    "    print(f\"  {sim:.4f} | {doc1[:40]}... ↔ {doc2[:40]}...\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "522db020-93c1-4018-a4a6-9f2e40373e62",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Matriz de similitudes guardada en: ../data/processed/similarity_matrix_rdd\n"
     ]
    }
   ],
   "source": [
    "# Guardar matriz ahora sí en nuestra carpeta de procesados\n",
    "output_sim = \"../data/processed/similarity_matrix_rdd\"\n",
    "if os.path.exists(output_sim):\n",
    "    shutil.rmtree(output_sim)\n",
    "\n",
    "rdd_similarities.saveAsTextFile(output_sim)\n",
    "print(f\"- Matriz de similitudes guardada en: {output_sim}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a8ce3ad0-cbe3-4c0a-8098-1230ae029831",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844a20d6-6cee-4125-b24d-5bc5fc1bb264",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
