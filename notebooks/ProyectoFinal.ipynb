{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3abd3904-91ae-4fb9-88e3-d0601a125c00",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ya están descargadas\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Using incubator modules: jdk.incubator.vector\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "25/12/09 00:46:16 WARN Utils: Your hostname, DESKTOP-6OAF9F9, resolves to a loopback address: 127.0.1.1; using 172.28.59.214 instead (on interface eth0)\n",
      "25/12/09 00:46:19 WARN Utils: Set SPARK_LOCAL_IP if you need to bind to another address\n",
      "Using Spark's default log4j profile: org/apache/spark/log4j2-defaults.properties\n",
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "25/12/09 00:47:17 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "#Importaciones y configuración inicial\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.ml.linalg import Vectors, DenseVector\n",
    "import os\n",
    "import shutil\n",
    "import re\n",
    "import math\n",
    "import ast\n",
    "from itertools import combinations\n",
    "import nltk\n",
    "\n",
    "# Descargar stopwords de NLTK y de ahí las usamos para hacer nuestro paso 3\n",
    "try:\n",
    "    nltk.data.find('corpora/stopwords')\n",
    "    print(\"Ya están descargadas\")\n",
    "except LookupError:\n",
    "    nltk.download('stopwords', quiet=True)\n",
    "    print(\"Ya se descargaron las stopwords\")\n",
    "\n",
    "# Inicializar Spark\n",
    "spark = SparkSession.builder \\\n",
    "    .appName(\"ProyectoSparkie-ProyectoFinal\") \\\n",
    "    .master(\"local[*]\") \\\n",
    "    .getOrCreate()\n",
    "\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "96bb3cbf-c8fe-411f-8667-b0e879314b65",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libros cargados: 99\n"
     ]
    }
   ],
   "source": [
    "#PASO 3 - Cargar libros ya sin los encabezados del Gunterberg y preparar datos\n",
    "#======= Variables:\n",
    "# rdd_libros: RDD donde guardamos los libros y en la que trabajaremos\n",
    "# num_libros: Guardamos cuantos libros son y usarlo en calculos posteriores\n",
    "# rdd_lower: RDD de los libros transormados en minusculas\n",
    "# rdd_clean: RDD donde guardamos las palabras minusculas para quitarles los caracteres y simbolos innecesarios\n",
    "# rdd_tokens: RDD donde ya están los textos tokenizados \n",
    "\n",
    "# Cargar los libros desde la carpeta raw en la que guardamos nuestros libros (Cambiar esta parte si cambiamos de lugar los libros)\n",
    "base_path = os.path.abspath(\"../data/raw\")\n",
    "rdd_libros = sc.wholeTextFiles(base_path)\n",
    "num_libros = rdd_libros.count()\n",
    "\n",
    "print(f\"Libros cargados: {num_libros}\") # Para confirmar que la variable funciona y cuantos son. Lo podemos eliminar.\n",
    "\n",
    "# Convertir a minúsculas: \n",
    "rdd_lower = rdd_libros.map(lambda x: (x[0], x[1].lower()))\n",
    "\n",
    "# Función para limpiar caracteres y símbolos innecesarios\n",
    "def limpiar(texto):\n",
    "    return re.sub(r\"[^a-z0-9\\s]\", \" \", texto)\n",
    "\n",
    "rdd_clean = rdd_lower.map(lambda x: (x[0], limpiar(x[1])))\n",
    "\n",
    "# Tokenización\n",
    "rdd_tokens = rdd_clean.flatMap(lambda x: [(x[0], t) for t in x[1].split()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "fa43d204-5f73-4bec-a8e0-3a62614202aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Filtrar stopwords y preparar datos\n",
    "# ========= Variables:\n",
    "# stop = Lista de stopwords sacadas de librería nltk\n",
    "# stop_b = broadcast de stop\n",
    "# rdd_filtrado = RDD con TODOS los filtros aplicados.\n",
    "# rdd_doc_token = Nombres de los documentos\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Cargar y broadcast stopwords\n",
    "stop = set(stopwords.words(\"english\"))\n",
    "stop_b = sc.broadcast(stop)\n",
    "\n",
    "# Función para quitar palabras menores a 3\n",
    "def es_valida(palabra):\n",
    "    return palabra not in stop_b.value and len(palabra) > 2\n",
    "\n",
    "# Filtrar tokens\n",
    "rdd_filtrado = rdd_tokens.filter(lambda x: es_valida(x[1]))\n",
    "\n",
    "# Extraer nombre del documento\n",
    "def docname(path):\n",
    "    return os.path.basename(path)\n",
    "\n",
    "rdd_doc_token = rdd_filtrado.map(lambda x: (docname(x[0]), x[1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124a38bc-51ea-465f-9ebf-998649bd9eb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "25/12/09 01:00:45 WARN Executor: Issue communicating with driver in heartbeater]\n",
      "org.apache.spark.rpc.RpcTimeoutException: Future timed out after [10000 milliseconds]. This timeout is controlled by spark.executor.heartbeatInterval\n",
      "\tat org.apache.spark.rpc.RpcTimeout.org$apache$spark$rpc$RpcTimeout$$createRpcTimeoutException(RpcTimeout.scala:47)\n",
      "\tat org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:62)\n",
      "\tat org.apache.spark.rpc.RpcTimeout$$anonfun$addMessageIfTimeout$1.applyOrElse(RpcTimeout.scala:58)\n",
      "\tat scala.runtime.AbstractPartialFunction.apply(AbstractPartialFunction.scala:35)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:76)\n",
      "\tat org.apache.spark.rpc.RpcEndpointRef.askSync(RpcEndpointRef.scala:101)\n",
      "\tat org.apache.spark.executor.Executor.reportHeartBeat(Executor.scala:1292)\n",
      "\tat org.apache.spark.executor.Executor.$anonfun$heartbeater$1(Executor.scala:307)\n",
      "\tat scala.runtime.java8.JFunction0$mcV$sp.apply(JFunction0$mcV$sp.scala:18)\n",
      "\tat org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1937)\n",
      "\tat org.apache.spark.Heartbeater$$anon$1.run(Heartbeater.scala:46)\n",
      "\tat java.base/java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:572)\n",
      "\tat java.base/java.util.concurrent.FutureTask.runAndReset(FutureTask.java:358)\n",
      "\tat java.base/java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:305)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1144)\n",
      "\tat java.base/java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:642)\n",
      "\tat java.base/java.lang.Thread.run(Thread.java:1583)\n",
      "Caused by: java.util.concurrent.TimeoutException: Future timed out after [10000 milliseconds]\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.tryAwait0(Promise.scala:248)\n",
      "\tat scala.concurrent.impl.Promise$DefaultPromise.result(Promise.scala:261)\n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResultNoSparkExceptionConversion(SparkThreadUtils.scala:61)\n",
      "\tat org.apache.spark.util.SparkThreadUtils$.awaitResult(SparkThreadUtils.scala:45)\n",
      "\tat org.apache.spark.util.ThreadUtils$.awaitResult(ThreadUtils.scala:342)\n",
      "\tat org.apache.spark.rpc.RpcTimeout.awaitResult(RpcTimeout.scala:75)\n",
      "\t... 12 more\n",
      "25/12/09 01:00:55 WARN NettyRpcEnv: Ignored message: HeartbeatResponse(false) 2]\n",
      "                                                                                "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Vocabulario guardado en: ../data/processed/vocabulario_rdd\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR:root:KeyboardInterrupt while sending command.                 (0 + 2) / 2]\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/tlaloc/proyecto-sparkie/venv/lib/python3.12/site-packages/py4j/java_gateway.py\", line 1038, in send_command\n",
      "    response = connection.send_command(command)\n",
      "               ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/home/tlaloc/proyecto-sparkie/venv/lib/python3.12/site-packages/py4j/clientserver.py\", line 535, in send_command\n",
      "    answer = smart_decode(self.stream.readline()[:-1])\n",
      "                          ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/lib/python3.12/socket.py\", line 707, in readinto\n",
      "    return self._sock.recv_into(b)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^\n",
      "KeyboardInterrupt\n",
      "[Stage 3:>                                                          (0 + 2) / 2]"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[4]\u001b[39m\u001b[32m, line 19\u001b[39m\n\u001b[32m     17\u001b[39m \u001b[38;5;66;03m# Vocabulario global (todas las palabras únicas del corpus)\u001b[39;00m\n\u001b[32m     18\u001b[39m vocab_global = rdd_doc_token.map(\u001b[38;5;28;01mlambda\u001b[39;00m x: x[\u001b[32m1\u001b[39m]).distinct()\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mVocabulario global: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[43mvocab_global\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcount\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m palabras únicas\u001b[39m\u001b[33m\"\u001b[39m) \u001b[38;5;66;03m#Confirmamos\u001b[39;00m\n\u001b[32m     21\u001b[39m \u001b[38;5;66;03m# Guardar vocabulario global\u001b[39;00m\n\u001b[32m     22\u001b[39m output_global = \u001b[33m\"\u001b[39m\u001b[33m../data/processed/vocabulario_unico_rdd\u001b[39m\u001b[33m\"\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/proyecto-sparkie/venv/lib/python3.12/site-packages/pyspark/core/rdd.py:2183\u001b[39m, in \u001b[36mRDD.count\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   2162\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mcount\u001b[39m(\u001b[38;5;28mself\u001b[39m) -> \u001b[38;5;28mint\u001b[39m:\n\u001b[32m   2163\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   2164\u001b[39m \u001b[33;03m    Return the number of elements in this RDD.\u001b[39;00m\n\u001b[32m   2165\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   2181\u001b[39m \u001b[33;03m    3\u001b[39;00m\n\u001b[32m   2182\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2183\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmapPartitions\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[32;43m1\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43m_\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43msum\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/proyecto-sparkie/venv/lib/python3.12/site-packages/pyspark/core/rdd.py:2158\u001b[39m, in \u001b[36mRDD.sum\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   2137\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34msum\u001b[39m(\u001b[38;5;28mself\u001b[39m: \u001b[33m\"\u001b[39m\u001b[33mRDD[NumberOrArray]\u001b[39m\u001b[33m\"\u001b[39m) -> \u001b[33m\"\u001b[39m\u001b[33mNumberOrArray\u001b[39m\u001b[33m\"\u001b[39m:\n\u001b[32m   2138\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"\u001b[39;00m\n\u001b[32m   2139\u001b[39m \u001b[33;03m    Add up the elements in this RDD.\u001b[39;00m\n\u001b[32m   2140\u001b[39m \n\u001b[32m   (...)\u001b[39m\u001b[32m   2156\u001b[39m \u001b[33;03m    6.0\u001b[39;00m\n\u001b[32m   2157\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m2158\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmapPartitions\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfold\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# type: ignore[return-value]\u001b[39;49;00m\n\u001b[32m   2159\u001b[39m \u001b[43m        \u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moperator\u001b[49m\u001b[43m.\u001b[49m\u001b[43madd\u001b[49m\n\u001b[32m   2160\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/proyecto-sparkie/venv/lib/python3.12/site-packages/pyspark/core/rdd.py:1911\u001b[39m, in \u001b[36mRDD.fold\u001b[39m\u001b[34m(self, zeroValue, op)\u001b[39m\n\u001b[32m   1906\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m acc\n\u001b[32m   1908\u001b[39m \u001b[38;5;66;03m# collecting result of mapPartitions here ensures that the copy of\u001b[39;00m\n\u001b[32m   1909\u001b[39m \u001b[38;5;66;03m# zeroValue provided to each partition is unique from the one provided\u001b[39;00m\n\u001b[32m   1910\u001b[39m \u001b[38;5;66;03m# to the final reduce call\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1911\u001b[39m vals = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mmapPartitions\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcollect\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1912\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m reduce(op, vals, zeroValue)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/proyecto-sparkie/venv/lib/python3.12/site-packages/pyspark/core/rdd.py:1700\u001b[39m, in \u001b[36mRDD.collect\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1698\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m SCCallSiteSync(\u001b[38;5;28mself\u001b[39m.context):\n\u001b[32m   1699\u001b[39m     \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.ctx._jvm \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1700\u001b[39m     sock_info = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mctx\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_jvm\u001b[49m\u001b[43m.\u001b[49m\u001b[43mPythonRDD\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcollectAndServe\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_jrdd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrdd\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1701\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mlist\u001b[39m(_load_from_socket(sock_info, \u001b[38;5;28mself\u001b[39m._jrdd_deserializer))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/proyecto-sparkie/venv/lib/python3.12/site-packages/py4j/java_gateway.py:1361\u001b[39m, in \u001b[36mJavaMember.__call__\u001b[39m\u001b[34m(self, *args)\u001b[39m\n\u001b[32m   1354\u001b[39m args_command, temp_args = \u001b[38;5;28mself\u001b[39m._build_args(*args)\n\u001b[32m   1356\u001b[39m command = proto.CALL_COMMAND_NAME +\\\n\u001b[32m   1357\u001b[39m     \u001b[38;5;28mself\u001b[39m.command_header +\\\n\u001b[32m   1358\u001b[39m     args_command +\\\n\u001b[32m   1359\u001b[39m     proto.END_COMMAND_PART\n\u001b[32m-> \u001b[39m\u001b[32m1361\u001b[39m answer = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mgateway_client\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1362\u001b[39m return_value = get_return_value(\n\u001b[32m   1363\u001b[39m     answer, \u001b[38;5;28mself\u001b[39m.gateway_client, \u001b[38;5;28mself\u001b[39m.target_id, \u001b[38;5;28mself\u001b[39m.name)\n\u001b[32m   1365\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/proyecto-sparkie/venv/lib/python3.12/site-packages/py4j/java_gateway.py:1038\u001b[39m, in \u001b[36mGatewayClient.send_command\u001b[39m\u001b[34m(self, command, retry, binary)\u001b[39m\n\u001b[32m   1036\u001b[39m connection = \u001b[38;5;28mself\u001b[39m._get_connection()\n\u001b[32m   1037\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1038\u001b[39m     response = \u001b[43mconnection\u001b[49m\u001b[43m.\u001b[49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1039\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m binary:\n\u001b[32m   1040\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m response, \u001b[38;5;28mself\u001b[39m._create_connection_guard(connection)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/proyecto-sparkie/venv/lib/python3.12/site-packages/py4j/clientserver.py:535\u001b[39m, in \u001b[36mClientServerConnection.send_command\u001b[39m\u001b[34m(self, command)\u001b[39m\n\u001b[32m    533\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m    534\u001b[39m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m535\u001b[39m         answer = smart_decode(\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mstream\u001b[49m\u001b[43m.\u001b[49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[:-\u001b[32m1\u001b[39m])\n\u001b[32m    536\u001b[39m         logger.debug(\u001b[33m\"\u001b[39m\u001b[33mAnswer received: \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[33m\"\u001b[39m.format(answer))\n\u001b[32m    537\u001b[39m         \u001b[38;5;66;03m# Happens when a the other end is dead. There might be an empty\u001b[39;00m\n\u001b[32m    538\u001b[39m         \u001b[38;5;66;03m# answer before the socket raises an error.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/lib/python3.12/socket.py:707\u001b[39m, in \u001b[36mSocketIO.readinto\u001b[39m\u001b[34m(self, b)\u001b[39m\n\u001b[32m    705\u001b[39m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[32m    706\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m707\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_sock\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    708\u001b[39m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[32m    709\u001b[39m         \u001b[38;5;28mself\u001b[39m._timeout_occurred = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "# Crear vocabulario y guardar Vocabulario por documento (palabra única por documento)\n",
    "# ======== Variables:\n",
    "# rdd_vocab = rdd de palabras unicas por documento\n",
    "# output_vocab = string de la ubicación de nuestro vocabulario y donde guardaremos el rdd_vocab\n",
    "# vocab_global = Vocabulario global de todos los documentos\n",
    "# output_global = string con la ubicación donde guardaremos output_global\n",
    "\n",
    "rdd_vocab = rdd_doc_token.distinct()\n",
    "\n",
    "# Guardar vocabulario por documento\n",
    "output_vocab = \"../data/processed/vocabulario_rdd\" # ubicación de nuestros vocabularios procesados, cambiar si movemos la ubicación\n",
    "if os.path.exists(output_vocab):\n",
    "    shutil.rmtree(output_vocab)\n",
    "rdd_vocab.saveAsTextFile(output_vocab)\n",
    "print(f\"\\nVocabulario guardado en: {output_vocab}\") # -> Mensaje para confirmar\n",
    "\n",
    "# Vocabulario global (todas las palabras únicas del corpus)\n",
    "vocab_global = rdd_doc_token.map(lambda x: x[1]).distinct()\n",
    "print(f\"Vocabulario global: {vocab_global.count()} palabras únicas\") #Confirmamos\n",
    "\n",
    "# Guardar vocabulario global\n",
    "output_global = \"../data/processed/vocabulario_unico_rdd\"\n",
    "if os.path.exists(output_global):\n",
    "    shutil.rmtree(output_global)\n",
    "vocab_global.saveAsTextFile(output_global)\n",
    "print(f\"Vocabulario global guardado en: {output_global}\") #Confirmamos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71d03529-eca4-445f-baef-337650ed0ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular frecuencias de palabras\n",
    "# ============ Variables:\n",
    "# rdd_pairs = Duplas en formato ((documento, palabra), 1)\n",
    "# rdd_freq = rdd reducida para saber la frecuencia\n",
    "# output_freq = String de la ubicación donde guardaremos rdd_freq\n",
    "\n",
    "# Crear duplas ((documento, palabra), 1)\n",
    "rdd_pairs = rdd_doc_token.map(lambda x: ((x[0], x[1]), 1))\n",
    "\n",
    "# Reducir por clave para contar frecuencias\n",
    "rdd_freq = rdd_pairs.reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "# Esto lo usamos para saber si está bien así\n",
    "#print(f\"Frecuencias calculadas: {rdd_freq.count()} pares (documento, palabra)\")\n",
    "#print(\"\\n  Primeros 5 pares con frecuencias:\")\n",
    "#for item in rdd_freq.take(5):\n",
    "#    print(f\"    {item[0]}: {item[1]} apariciones\")\n",
    "\n",
    "# Guardar frecuencias\n",
    "output_freq = \"../data/processed/frecuencias_rdd\"\n",
    "if os.path.exists(output_freq):\n",
    "    shutil.rmtree(output_freq)\n",
    "rdd_freq.saveAsTextFile(output_freq)\n",
    "\n",
    "print(f\"\\nFrecuencias guardadas en: {output_freq}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e95067cb-49ed-441b-a2e5-ff155c46798a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Aquí inicia nuestro PASO 4 de crear matriz de similitud\n",
    "# ======== Variables:\n",
    "# rdd_parsed = tublas de frecuencias en (doc, palabra, freq) usando de referencia rdd_freq\n",
    "# doc_totals = total palabras por documento\n",
    "# rdd_doc_word_freq = rdd ((documento, palabra), frecuencia)\n",
    "# rdd_with_totals = joins con totales de rdd_doc_word_freq\n",
    "# rdd_tf = rdd con las TF\n",
    "\n",
    "# Función que convierte tupla ((doc, palabra), freq) a (doc, palabra, freq)\n",
    "def parse_freq_tuple(x):\n",
    "    return (x[0][0], x[0][1], x[1])\n",
    "\n",
    "# Parsear frecuencias \n",
    "rdd_parsed = rdd_freq.map(parse_freq_tuple)\n",
    "\n",
    "# Calcular total de palabras por documento\n",
    "doc_totals = rdd_parsed.map(lambda x: (x[0], x[2])) \\\n",
    "                       .reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "# print(f\"Total de palabras calculado para {doc_totals.count()} documentos\")\n",
    "\n",
    "# Calcular TF con formula: freq / total_palabras_doc\n",
    "\n",
    "rdd_doc_word_freq = rdd_parsed.map(lambda x: ((x[0], x[1]), x[2]))\n",
    "\n",
    "# Join con totales\n",
    "rdd_with_totals = rdd_doc_word_freq.map(lambda x: (x[0][0], (x[0][1], x[1]))) \\\n",
    "                                   .join(doc_totals) \\\n",
    "                                   .map(lambda x: ((x[0], x[1][0][0]), (x[1][0][1], x[1][1])))\n",
    "\n",
    "# Calcular TF\n",
    "rdd_tf = rdd_with_totals.map(lambda x: (x[0], x[1][0] / x[1][1]))\n",
    "\n",
    "print(\"TF (Term Frequency) calculado\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b63b0f4c-1478-4fd2-9418-bc0fe3b3deef",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aquí se calcula IDF\n",
    "# ====== Variables\n",
    "# total_docs = Documentos totales (Lo calculamos aopartir de doc_totals)\n",
    "# rdd_word_doc_count = Cantidad de documentos en los que aparece una palabra\n",
    "# rdd_idf = rdd con las idf calculadas\n",
    "\n",
    "# Contar total de documentos\n",
    "total_docs = doc_totals.count()\n",
    "print(f\"Total de documentos en el corpus: {total_docs}\")\n",
    "\n",
    "# Contar en cuántos documentos aparece cada palabra\n",
    "rdd_word_doc_count = rdd_parsed.map(lambda x: (x[1], x[0])) \\\n",
    "                               .distinct() \\\n",
    "                               .map(lambda x: (x[0], 1)) \\\n",
    "                               .reduceByKey(lambda a, b: a + b)\n",
    "\n",
    "# Calcular IDF: log(total_docs / docs_con_palabra)\n",
    "rdd_idf = rdd_word_doc_count.map(lambda x: (x[0], math.log(total_docs / x[1])))\n",
    "\n",
    "#print(\"IDF calculado\")\n",
    "#print(f\" Total de palabras únicas con IDF: {rdd_idf.count()}\")\n",
    "#print(f\" ejemplo de idf para ver como quedó: {rdd_idf.first()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a970b2fb-98eb-4b42-953a-ea16e36af82a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# En esta calculamos TF-IDF y ya guardar\n",
    "# rdd_tf_prep -> (palabra, (doc, tf)) = rdd preparando para hacer el calculo\n",
    "# rdd_tf_idf -> (palabra, ((doc, tf), idf)) = TF-IDF calculados\n",
    "# output_tfidf = String de ubicación de tfidf\n",
    "\n",
    "# Preparar TF: ((doc, palabra), tf) -> (palabra, (doc, tf))\n",
    "rdd_tf_prep = rdd_tf.map(lambda x: (x[0][1], (x[0][0], x[1])))\n",
    "\n",
    "# Join TF con IDF: (palabra, ((doc, tf), idf))\n",
    "rdd_tf_idf = rdd_tf_prep.join(rdd_idf) \\\n",
    "                        .map(lambda x: ((x[1][0][0], x[0]), x[1][0][1] * x[1][1]))\n",
    "\n",
    "#print(\"TF-IDF calculado\")\n",
    "#print(f\" Total de valores TF-IDF: {rdd_tf_idf.count()}\")\n",
    "#print(f\" Uno de ejemplo: {rdd_tf_idf.first()}\")\n",
    "\n",
    "# Guardar TF-IDF\n",
    "output_tfidf = \"../data/processed/tfidf_rdd\"\n",
    "\n",
    "if os.path.exists(output_tfidf):\n",
    "    shutil.rmtree(output_tfidf)\n",
    "rdd_tf_idf.saveAsTextFile(output_tfidf)\n",
    "print(f\"TF-IDF guardado en: {output_tfidf}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3c8e26a-8174-43e4-8755-07597649cf05",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Creamos vectores TF-IDF por documento\n",
    "# ================= == Variables\n",
    "# vocab_global_indexed = Vocabulario global por indice\n",
    "# vocab_dict = diccionario de datos\n",
    "# vocab_size = Contar numero de palabras\n",
    "\n",
    "# Crear vocabulario global con índices\n",
    "vocab_global_indexed = rdd_tf_idf.map(lambda x: x[0][1]).distinct().zipWithIndex()\n",
    "vocab_dict = vocab_global_indexed.collectAsMap()\n",
    "vocab_size = len(vocab_dict)\n",
    "\n",
    "#print(f\"Hay {vocab_size} palabras únicas\")\n",
    "\n",
    "# Convertir TF-IDF a formato (doc, [(índice, tfidf), ...])\n",
    "rdd_doc_vectors = rdd_tf_idf.map(lambda x: (x[0][0], (vocab_dict[x[0][1]], x[1]))) \\\n",
    "                            .groupByKey() \\\n",
    "                            .map(lambda x: (x[0], list(x[1])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e2ee859-c872-4258-813d-2df73d7c9146",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Función de similitud coseno\n",
    "def cosine_similarity_sparse(vec1, vec2):\n",
    "    \"\"\"\n",
    "    Calcula similitud coseno entre dos vectores dispersos\n",
    "    Parámetros:\n",
    "    - vec1, vec2: lista de tuplas [(índice, valor), ...]\n",
    "    Retorna:\n",
    "    - similitud coseno con valores de (0 a 1)\n",
    "    \"\"\"\n",
    "    # Convertir a diccionarios para búsqueda rápida\n",
    "    dict1 = dict(vec1)\n",
    "    dict2 = dict(vec2)\n",
    "    \n",
    "    # Producto punto\n",
    "    dot_product = sum(dict1.get(idx, 0) * dict2.get(idx, 0) \n",
    "                      for idx in set(dict1.keys()) | set(dict2.keys()))\n",
    "    \n",
    "    # Normas\n",
    "    norm1 = math.sqrt(sum(v**2 for v in dict1.values()))\n",
    "    norm2 = math.sqrt(sum(v**2 for v in dict2.values()))\n",
    "    \n",
    "    if norm1 == 0 or norm2 == 0:\n",
    "        return 0.0\n",
    "    \n",
    "    return dot_product / (norm1 * norm2)\n",
    "\n",
    "#print(\"Función de similitud coseno definida\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41358092-c1f9-4d85-a0d5-4dd8e91bb455",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aquí se calcula matriz de similitud\n",
    "# ========== Variables:\n",
    "# doc_vectors_list = lista de vectores por documento\n",
    "# \n",
    "# \n",
    "\n",
    "# Recolectar vectores\n",
    "doc_vectors_list = rdd_doc_vectors.collect()\n",
    "doc_names = [doc[0] for doc in doc_vectors_list]\n",
    "total_pairs = len(doc_names) * (len(doc_names) - 1) // 2\n",
    "\n",
    "#print(f\"Total de pares a calcular: {total_pairs}\")\n",
    "\n",
    "# Calcular similitudes\n",
    "similarities = []\n",
    "for i, (doc1, vec1) in enumerate(doc_vectors_list):\n",
    "    for doc2, vec2 in doc_vectors_list[i+1:]:\n",
    "        sim = cosine_similarity_sparse(vec1, vec2)\n",
    "        similarities.append((doc1, doc2, sim))\n",
    "    \n",
    "    if (i + 1) % 10 == 0:\n",
    "        print(f\"  Procesados: {i+1}/{len(doc_vectors_list)} documentos...\")\n",
    "\n",
    "# Convertir a RDD\n",
    "rdd_similarities = sc.parallelize(similarities)\n",
    "\n",
    "print(f\"\\nSimilitudes calculadas: {rdd_similarities.count()} pares\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bdaebbb-12df-43eb-bfe6-49c552ba229c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Guardamos matriz de similitud\n",
    "output_sim = \"../data/processed/similarity_matrix_rdd\"\n",
    "if os.path.exists(output_sim):\n",
    "    shutil.rmtree(output_sim)\n",
    "\n",
    "rdd_similarities.saveAsTextFile(output_sim)\n",
    "print(f\"Matriz de similitudes guardada en: {output_sim}\")\n",
    "\n",
    "# Extraer lista de libros disponibles para usar en funciones posteriores\n",
    "libros_disponibles = rdd_similarities.flatMap(lambda x: [x[0], x[1]]).distinct().collect()\n",
    "print(f\"Lista de libros disponibles: {len(libros_disponibles)} libros\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2f308c0-065d-4643-946d-1bd88bab98e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aquí iniciamos nuestro PASO 5 - Función de búsqueda de libros\n",
    "\n",
    "def buscar_libro(nombre_libro, libros_disponibles):\n",
    "    \"\"\"\n",
    "    Busca un libro en la colección\n",
    "    Parámetros:\n",
    "    - nombre_libro: nombre del libro (búsqueda parcial permitida)\n",
    "    - libros_disponibles: lista de todos los libros\n",
    "    Retorna:\n",
    "    - nombre completo del libro si existe\n",
    "    - None si no existe o hay ambigüedad\n",
    "    \"\"\"\n",
    "    nombre_lower = nombre_libro.lower()\n",
    "    \n",
    "    # Búsqueda exacta\n",
    "    for libro in libros_disponibles:\n",
    "        if libro.lower() == nombre_lower:\n",
    "            return libro\n",
    "    \n",
    "    # Búsqueda parcial\n",
    "    coincidencias = [libro for libro in libros_disponibles \n",
    "                     if nombre_lower in libro.lower()]\n",
    "    \n",
    "    if len(coincidencias) == 1:\n",
    "        print(f\"Libro encontrado: {coincidencias[0]}\")\n",
    "        return coincidencias[0]\n",
    "    elif len(coincidencias) > 1:\n",
    "        print(f\"Se encontraron {len(coincidencias)} libros que coinciden:\")\n",
    "        for i, libro in enumerate(coincidencias[:5], 1):\n",
    "            print(f\"   {i}. {libro}\")\n",
    "        if len(coincidencias) > 5:\n",
    "            print(f\"   ... y {len(coincidencias) - 5} más\")\n",
    "        print(\"\\n Por favor, sea más específico con el nombre :c\")\n",
    "        return None\n",
    "    else:\n",
    "        print(f\"Error: El libro '{nombre_libro}' no existe en la colección\")\n",
    "        print(\"\\n Sugerencias (libros similares):\")\n",
    "        palabras = nombre_lower.split()\n",
    "        sugerencias = []\n",
    "        for libro in libros_disponibles:\n",
    "            libro_lower = libro.lower()\n",
    "            if any(palabra in libro_lower for palabra in palabras):\n",
    "                sugerencias.append(libro)\n",
    "        \n",
    "        for i, libro in enumerate(sugerencias[:5], 1):\n",
    "            print(f\"   {i}. {libro}\")\n",
    "        \n",
    "        return None\n",
    "\n",
    "# print(\"Función de búsqueda de libros definida\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "986a42f1-e60c-4e1e-8d3d-f6e531b04239",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paso 5\n",
    "\n",
    "def recomendar_libros(libro_favorito, n):\n",
    "    \"\"\"\n",
    "    Recomienda N libros más similares al libro favorito del usuario.\n",
    "    \n",
    "    Parámetros:\n",
    "    - libro_favorito: nombre del libro (str)\n",
    "    - n: cantidad de libros a recomendar (int, default=5)\n",
    "    \n",
    "    Imprime:\n",
    "    - N libros recomendados con su similitud\n",
    "    \"\"\"\n",
    "    # Validar que el libro existe\n",
    "    libro_completo = buscar_libro(libro_favorito, libros_disponibles)\n",
    "    \n",
    "    if libro_completo is None:\n",
    "        return\n",
    "    \n",
    "    print(f\"\\nBuscando libros similares a: {libro_completo}\")\n",
    "    print(f\"   Cantidad solicitada: {n} recomendaciones\\n\")\n",
    "    \n",
    "    # Filtrar similitudes donde aparece el libro\n",
    "    similitudes_libro = rdd_similarities.filter(\n",
    "        lambda x: x[0] == libro_completo or x[1] == libro_completo\n",
    "    )\n",
    "    \n",
    "    # Reformatear: (libro_recomendado, similitud)\n",
    "    recomendaciones = similitudes_libro.map(\n",
    "        lambda x: (x[1], x[2]) if x[0] == libro_completo else (x[0], x[2])\n",
    "    )\n",
    "    \n",
    "    # Ordenar por similitud mayor a menor\n",
    "    top_recomendaciones = recomendaciones.sortBy(lambda x: x[1], ascending=False).take(n)\n",
    "    \n",
    "    # Imprimir resultados\n",
    "    print(\"Recomendaciones:\")\n",
    "    for i, (libro, similitud) in enumerate(top_recomendaciones, 1):\n",
    "        print(f\"   {i}. [{similitud:.4f}] {libro}\")\n",
    "\n",
    "#print(\"Función de recomendación definida\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "462829b7-5d85-4e35-84b5-c53f2ddc7c53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PASO 6 - Función de palabras descriptivas\n",
    "\n",
    "def palabras_descriptivas(libro, M):\n",
    "    \"\"\"\n",
    "    Muestra las M palabras más representativas del libro.\n",
    "    Basado en frecuencia de aparición.\n",
    "    \n",
    "    Parámetros:\n",
    "    - libro: nombre del libro (str)\n",
    "    - M: cantidad de palabras a mostrar (int, default=10)\n",
    "    \n",
    "    Imprime:\n",
    "    - M palabras más frecuentes con su número de apariciones\n",
    "    \"\"\"\n",
    "    libro_ok = buscar_libro(libro, libros_disponibles)\n",
    "    \n",
    "    if libro_ok is None:\n",
    "        return\n",
    "    \n",
    "    print(f\"\\n Palabras clave de: {libro_ok}\\n\")\n",
    "    \n",
    "    # Filtrar solo ese documento del RDD de frecuencias\n",
    "    palabras = (rdd_freq\n",
    "        .filter(lambda x: x[0][0] == libro_ok)  # x[0] = (doc, palabra)\n",
    "        .map(lambda x: (x[0][1], x[1]))         # (palabra, freq)\n",
    "        .sortBy(lambda x: x[1], ascending=False)\n",
    "        .take(M)\n",
    "    )\n",
    "    \n",
    "    # Imprimir resultados\n",
    "    print(\"- Palabras que describen el libro:\\n\")\n",
    "    for i, (palabra, freq) in enumerate(palabras, 1):\n",
    "        print(f\"   {i}. {palabra:20} → {freq:5} apariciones\")\n",
    "\n",
    "#print(\"Función de palabras descriptivas definida\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d463eeb8-1c3a-4e6a-b917-399b1c7ab016",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ejemplos\n",
    "recomendar_libros(\"The_Expedition_of_Humphry_Clinker\", n=5)\n",
    "palabras_descriptivas(\"The Odyssey\", M=15)\n",
    "recomendar_libros(\"Harry Potter\", n=5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d73d4311-bb43-49fe-98ff-0ac5e64085fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
